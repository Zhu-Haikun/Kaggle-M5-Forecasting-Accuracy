{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M5 时序预测\n",
    "\n",
    "\n",
    "本 NoteBook 是 Kaggle M5 时序预测比赛的 Baseline，主要使用机器学习的建模方式进行时序的建模和预测。\n",
    "\n",
    "\n",
    "- **BaseLine步骤**：    \n",
    "                        1. 数据分析 EDA\n",
    "                        2. 特征工程\n",
    "                        3. 模型训练\n",
    "                        4. 线下验证\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、数据分析\n",
    "## 1.1 加载库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-feece9b8429a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# import feature engineering lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m  \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import feature engineering lib\n",
    "import sys\n",
    "import lightgbm as lgb\n",
    "from  datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> github免费版对上传数据有限制，不能上传大于100MB的文件，需要将原有128MB数据集进行压缩，压缩格式使用bz2，在此格式下，pyspark可以直接读取数据集。\n",
    "\n",
    "- Mac OS下，通过Terminal使用如下命令将大于100MB的数据集压缩\n",
    "\n",
    "```shell\n",
    "$ bzip2 -z sales_train_validation.csv\n",
    "$ bzip2 -z sell_prices.csv\n",
    "```\n",
    "\n",
    "> **注意：使用此命令后会删除被压缩的文件**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**该部分需要完善**  \n",
    "为了节约时间，我们直接对我们后面建模有用的结果进行分析，关于数据的详细分析可以参考 [EDA](https://www.kaggle.com/headsortails/back-to-predict-the-future-interactive-m5-eda)\n",
    "\n",
    "- 查看 sales 数据前几行\n",
    "- 查看 sales 数据聚合结果趋势\n",
    "- 查看 sales 数据标签分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sale_data = pd.read_csv('./m5-forecasting-accuracy/sales_train_validation.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d_1</th>\n",
       "      <th>d_2</th>\n",
       "      <th>d_3</th>\n",
       "      <th>d_4</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1904</th>\n",
       "      <th>d_1905</th>\n",
       "      <th>d_1906</th>\n",
       "      <th>d_1907</th>\n",
       "      <th>d_1908</th>\n",
       "      <th>d_1909</th>\n",
       "      <th>d_1910</th>\n",
       "      <th>d_1911</th>\n",
       "      <th>d_1912</th>\n",
       "      <th>d_1913</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_003</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HOBBIES_1_006_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_006</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HOBBIES_1_007_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_007</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HOBBIES_1_008_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_008</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HOBBIES_1_009_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_009</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_010</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 1919 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_validation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "1  HOBBIES_1_002_CA_1_validation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
       "2  HOBBIES_1_003_CA_1_validation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
       "3  HOBBIES_1_004_CA_1_validation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
       "4  HOBBIES_1_005_CA_1_validation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
       "5  HOBBIES_1_006_CA_1_validation  HOBBIES_1_006  HOBBIES_1  HOBBIES     CA_1   \n",
       "6  HOBBIES_1_007_CA_1_validation  HOBBIES_1_007  HOBBIES_1  HOBBIES     CA_1   \n",
       "7  HOBBIES_1_008_CA_1_validation  HOBBIES_1_008  HOBBIES_1  HOBBIES     CA_1   \n",
       "8  HOBBIES_1_009_CA_1_validation  HOBBIES_1_009  HOBBIES_1  HOBBIES     CA_1   \n",
       "9  HOBBIES_1_010_CA_1_validation  HOBBIES_1_010  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id  d_1  d_2  d_3  d_4  ...  d_1904  d_1905  d_1906  d_1907  d_1908  \\\n",
       "0       CA    0    0    0    0  ...       1       3       0       1       1   \n",
       "1       CA    0    0    0    0  ...       0       0       0       0       0   \n",
       "2       CA    0    0    0    0  ...       2       1       2       1       1   \n",
       "3       CA    0    0    0    0  ...       1       0       5       4       1   \n",
       "4       CA    0    0    0    0  ...       2       1       1       0       1   \n",
       "5       CA    0    0    0    0  ...       0       1       0       1       0   \n",
       "6       CA    0    0    0    0  ...       0       0       0       1       0   \n",
       "7       CA   12   15    0    0  ...       0       0       1      37       3   \n",
       "8       CA    2    0    7    3  ...       0       0       1       1       6   \n",
       "9       CA    0    0    1    0  ...       1       0       0       0       0   \n",
       "\n",
       "   d_1909  d_1910  d_1911  d_1912  d_1913  \n",
       "0       1       3       0       1       1  \n",
       "1       1       0       0       0       0  \n",
       "2       1       0       1       1       1  \n",
       "3       0       1       3       7       2  \n",
       "4       1       2       2       2       4  \n",
       "5       0       0       2       0       0  \n",
       "6       1       0       0       1       1  \n",
       "7       4       6       3       2       1  \n",
       "8       0       0       0       0       0  \n",
       "9       0       0       2       0       2  \n",
       "\n",
       "[10 rows x 1919 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sale_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_data = sale_data[[f'd_{day}' for day in range(1,1914)]]\n",
    "total_sum = np.sum(day_data,axis=0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(total_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(day_data[day_data<100].values.reshape(-1),bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、特征工程\n",
    "\n",
    "选定机器学习的建模方案，核心思想是对时间序列抽取窗口特征。\n",
    "> 需要补充什么叫做对时间序列抽取窗口特征\n",
    "\n",
    "需要抽取的窗口特征：\n",
    "\n",
    "- 前7天\n",
    "- 前28天\n",
    "- 前7天均值\n",
    "- 前28天均值\n",
    "\n",
    "抽取的窗口特征需要关联其他维度信息\n",
    "\n",
    "- 日期\n",
    "- 价格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 描述该函数的作用\n",
    "def create_train_data(train_start=750,test_start=1800,is_train=True):\n",
    "    # 基本参数\n",
    "    PRICE_DTYPES = {\"store_id\": \"category\", \"item_id\": \"category\", \"wm_yr_wk\": \"int16\",\"sell_price\":\"float32\" }\n",
    "    CAL_DTYPES={\"event_name_1\": \"category\", \"event_name_2\": \"category\", \"event_type_1\": \"category\", \n",
    "            \"event_type_2\": \"category\", \"weekday\": \"category\", 'wm_yr_wk': 'int16', \"wday\": \"int16\",\n",
    "            \"month\": \"int16\", \"year\": \"int16\", \"snap_CA\": \"float32\", 'snap_TX': 'float32', 'snap_WI': 'float32' }\n",
    "\n",
    "    start_day = train_start if is_train else test_start\n",
    "    numcols = [f\"d_{day}\" for day in range(start_day,1914)]\n",
    "    catcols = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\n",
    "    SALE_DTYPES = {numcol:\"float32\" for numcol in numcols} \n",
    "    SALE_DTYPES.update({col: \"category\" for col in catcols if col != \"id\"})\n",
    "\n",
    "    # 加载price数据\n",
    "    price_data = pd.read_csv('./m5-forecasting-accuracy/sell_prices.csv',dtype=PRICE_DTYPES)\n",
    "    # 加载cal数据\n",
    "    cal_data = pd.read_csv('./m5-forecasting-accuracy/calendar.csv',dtype=CAL_DTYPES)\n",
    "    # 加载sale数据\n",
    "    sale_data = pd.read_csv('./m5-forecasting-accuracy/sales_train_validation.csv',dtype=SALE_DTYPES,usecols=catcols+numcols)\n",
    "\n",
    "\n",
    "    # 类别标签转换\n",
    "    for col, col_dtype in PRICE_DTYPES.items():\n",
    "        if col_dtype == \"category\":\n",
    "            price_data[col] = price_data[col].cat.codes.astype(\"int16\")\n",
    "            price_data[col] -= price_data[col].min()\n",
    "\n",
    "    cal_data[\"date\"] = pd.to_datetime(cal_data[\"date\"])\n",
    "    for col, col_dtype in CAL_DTYPES.items():\n",
    "        if col_dtype == \"category\":\n",
    "            cal_data[col] = cal_data[col].cat.codes.astype(\"int16\")\n",
    "            cal_data[col] -= cal_data[col].min()\n",
    "\n",
    "\n",
    "    for col in catcols:\n",
    "        if col != \"id\":\n",
    "            sale_data[col] = sale_data[col].cat.codes.astype(\"int16\")\n",
    "            sale_data[col] -= sale_data[col].min()\n",
    "\n",
    "    # 注意提交格式里有一部分为空\n",
    "    if not is_train:\n",
    "        for day in range(1913+1, 1913+ 2*28 +1):\n",
    "            sale_data[f\"d_{day}\"] = np.nan\n",
    "\n",
    "    sale_data = pd.melt(sale_data,\n",
    "            id_vars = catcols,\n",
    "            value_vars = [col for col in sale_data.columns if col.startswith(\"d_\")],\n",
    "            var_name = \"d\",\n",
    "            value_name = \"sales\")\n",
    "    sale_data = sale_data.merge(cal_data, on= \"d\", copy = False)\n",
    "    sale_data = sale_data.merge(price_data, on = [\"store_id\", \"item_id\", \"wm_yr_wk\"], copy = False)\n",
    "    return sale_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 描述该函数的作用\n",
    "def create_feature(sale_data, is_train=True, day=None):\n",
    "    # 可以在这里加入更多的特征抽取方法\n",
    "    # 获取7天前的数据，28天前的数据\n",
    "    lags = [7, 28]\n",
    "    lag_cols = [f\"lag_{lag}\" for lag in lags ]\n",
    "\n",
    "    # 如果是测试集只需要计算一天的特征，减少计算量\n",
    "    # 注意训练集和测试集特征生成要一致\n",
    "    if is_train:\n",
    "        for lag, lag_col in zip(lags, lag_cols):\n",
    "            sale_data[lag_col] = sale_data[[\"id\",\"sales\"]].groupby(\"id\")[\"sales\"].shift(lag)\n",
    "    else:\n",
    "        for lag, lag_col in zip(lags, lag_cols):\n",
    "            sale_data.loc[sale_data.date == day, lag_col] = sale_data.loc[sale_data.date ==day-timedelta(days=lag), 'sales'].values  \n",
    "\n",
    "\n",
    "    # 将获取7天前的数据，28天前的数据做移动平均\n",
    "    wins = [7, 28]\n",
    "\n",
    "    if is_train:\n",
    "        for win in wins :\n",
    "            for lag,lag_col in zip(lags, lag_cols):\n",
    "                sale_data[f\"rmean_{lag}_{win}\"] = sale_data[[\"id\", lag_col]].groupby(\"id\")[lag_col].transform(lambda x : x.rolling(win).mean())\n",
    "    else:\n",
    "        for win in wins:\n",
    "            for lag in lags:\n",
    "                df_window = sale_data[(sale_data.date <= day-timedelta(days=lag)) & (sale_data.date > day-timedelta(days=lag+win))]\n",
    "                df_window_grouped = df_window.groupby(\"id\").agg({'sales':'mean'}).reindex(sale_data.loc[sale_data.date==day,'id'])\n",
    "                sale_data.loc[sale_data.date == day,f\"rmean_{lag}_{win}\"] = df_window_grouped.sales.values   \n",
    "\n",
    "    # 处理时间特征\n",
    "    # 有的时间特征没有，通过datetime的方法自动生成\n",
    "    date_features = {\n",
    "            \"wday\": \"weekday\",\n",
    "            \"week\": \"weekofyear\",\n",
    "            \"month\": \"month\",\n",
    "            \"quarter\": \"quarter\",\n",
    "            \"year\": \"year\",\n",
    "            \"mday\": \"day\",\n",
    "        }\n",
    "\n",
    "    for date_feat_name, date_feat_func in date_features.items():\n",
    "        if date_feat_name in sale_data.columns:\n",
    "            sale_data[date_feat_name] = sale_data[date_feat_name].astype(\"int16\")\n",
    "        else:\n",
    "            sale_data[date_feat_name] = getattr(sale_data[\"date\"].dt, date_feat_func).astype(\"int16\")\n",
    "    return sale_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sale_data = create_train_data(train_start=350,is_train=True)\n",
    "sale_data = create_feature(sale_data)\n",
    "\n",
    "# 清洗数据，选择需要训练的数据\n",
    "sale_data.dropna(inplace=True)\n",
    "cat_feats = ['item_id', 'dept_id','store_id', 'cat_id', 'state_id'] + [\"event_name_1\", \"event_name_2\", \"event_type_1\", \"event_type_2\"]\n",
    "useless_cols = [\"id\", \"date\", \"sales\",\"d\", \"wm_yr_wk\", \"weekday\"]\n",
    "train_cols = sale_data.columns[~sale_data.columns.isin(useless_cols)]\n",
    "X_train = sale_data[train_cols]\n",
    "y_train = sale_data[\"sales\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、模型训练\n",
    "\n",
    "选择 LGB 模型进行模型的训练。\n",
    "\n",
    "[LGB安装](https://blog.csdn.net/u011433858/article/details/80402938)\n",
    "\n",
    "- 损失函数的选择\n",
    "- 预测时候的技巧\n",
    "\n",
    "<img src=\"tweedie.png\" style=\"width:300px;height:200px;float:center\">\n",
    "\n",
    "tweedie_variance_power 参数的选择 [1,2] 之间。\n",
    "\n",
    "LGB 模型是 GBDT 模型的变种，无法突然训练集的上界。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data,valid_data):\n",
    "    params = {\n",
    "        \"objective\" : \"tweedie\",\n",
    "        \"metric\" :\"rmse\",\n",
    "        \"force_row_wise\" : True,\n",
    "        \"learning_rate\" : 0.075,\n",
    "        \"sub_feature\" : 0.8,\n",
    "        \"sub_row\" : 0.75,\n",
    "        \"bagging_freq\" : 1,\n",
    "        \"lambda_l2\" : 0.1,\n",
    "        \"metric\": [\"rmse\"],\n",
    "        \"nthread\": 8,\n",
    "        \"tweedie_variance_power\":1.1,\n",
    "    'verbosity': 1,\n",
    "    'num_iterations' : 1500,\n",
    "    'num_leaves': 128,\n",
    "    \"min_data_in_leaf\": 104,\n",
    "    }\n",
    "\n",
    "    m_lgb = lgb.train(params, train_data, valid_sets = [valid_data], verbose_eval=50)\n",
    "\n",
    "    return m_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ensemble(train_cols,m_lgb):\n",
    "    date = datetime(2016,4, 25) \n",
    "    # alphas = [1.035, 1.03, 1.025, 1.02]\n",
    "    # alphas = [1.028, 1.023, 1.018]\n",
    "    alphas = [1.035, 1.03, 1.025]\n",
    "    weights = [1/len(alphas)]*len(alphas)\n",
    "    sub = 0.\n",
    "\n",
    "    test_data = create_train_data(is_train=False)\n",
    "\n",
    "    for icount, (alpha, weight) in enumerate(zip(alphas, weights)):\n",
    "\n",
    "        test_data_c = test_data.copy()\n",
    "        cols = [f\"F{i}\" for i in range(1,29)]\n",
    "\n",
    "\n",
    "        for i in range(0, 28):\n",
    "            day = date + timedelta(days=i)\n",
    "            print(i, day)\n",
    "            tst = test_data_c[(test_data_c.date >= day - timedelta(days=57)) & (test_data_c.date <= day)].copy()\n",
    "            tst = create_feature(tst,is_train=False, day=day)\n",
    "            tst = tst.loc[tst.date == day , train_cols]\n",
    "\n",
    "            test_data_c.loc[test_data_c.date == day, \"sales\"] = alpha*m_lgb.predict(tst)\n",
    "\n",
    "        # 改为提交数据的格式\n",
    "        test_sub = test_data_c.loc[test_data_c.date >= date, [\"id\", \"sales\"]].copy()\n",
    "        test_sub[\"F\"] = [f\"F{rank}\" for rank in test_sub.groupby(\"id\")[\"id\"].cumcount()+1]\n",
    "        test_sub = test_sub.set_index([\"id\", \"F\" ]).unstack()[\"sales\"][cols].reset_index()\n",
    "        test_sub.fillna(0., inplace = True)\n",
    "        test_sub.sort_values(\"id\", inplace = True)\n",
    "        test_sub.reset_index(drop=True, inplace = True)\n",
    "        test_sub.to_csv(f\"submission_{icount}.csv\",index=False)\n",
    "        if icount == 0 :\n",
    "            sub = test_sub\n",
    "            sub[cols] *= weight\n",
    "        else:\n",
    "            sub[cols] += test_sub[cols]*weight\n",
    "        print(icount, alpha, weight)\n",
    "    \n",
    "    sub2 = sub.copy()\n",
    "    # 把大于28天后的validation替换成evaluation\n",
    "    sub2[\"id\"] = sub2[\"id\"].str.replace(\"validation$\", \"evaluation\")\n",
    "    sub = pd.concat([sub, sub2], axis=0, sort=False)\n",
    "    sub.to_csv(\"submissionV3.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = lgb.Dataset(X_train, label = y_train, categorical_feature=cat_feats, free_raw_data=False)\n",
    "valid_inds = np.random.choice(len(X_train), 10000)\n",
    "valid_data = lgb.Dataset(X_train.iloc[valid_inds], label = y_train.iloc[valid_inds],categorical_feature=cat_feats, free_raw_data=False) \n",
    "\n",
    "m_lgb = train_model(train_data,valid_data) \n",
    "predict_ensemble(train_cols,m_lgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、线下验证\n",
    "\n",
    "WRMSSE 的评估方法和 RMSE 很不一致，我们需要拆分出么一条时间序列的权重到底是多少，一方面能帮助我们做线下验证，另一方面可以帮助我们思考能否使用自定义的损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import machine learning lib\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.sparse import csr_matrix\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换数据类型，减少内存占用空间\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics: \n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "data_pass = './m5-forecasting-accuracy/'\n",
    "\n",
    "# sale数据\n",
    "sales = pd.read_csv(data_pass+'sales_train_validation.csv')\n",
    "\n",
    "# 日期数据\n",
    "calendar = pd.read_csv(data_pass+'calendar.csv')\n",
    "calendar = reduce_mem_usage(calendar)\n",
    "\n",
    "# 价格数据\n",
    "sell_prices = pd.read_csv(data_pass+'sell_prices.csv')\n",
    "sell_prices = reduce_mem_usage(sell_prices)\n",
    "\n",
    "# 计算价格\n",
    "# 按照定义，只需要计算最近的 28 天售卖量（售卖数*价格），通过这个可以得到 weight\n",
    "# 可以不是 1914\n",
    "cols = [\"d_{}\".format(i) for i in range(1914-28, 1914)]\n",
    "data = sales[[\"id\", 'store_id', 'item_id'] + cols]\n",
    "\n",
    "# 从横表改为纵表\n",
    "data = data.melt(id_vars=[\"id\", 'store_id', 'item_id'], \n",
    "                 var_name=\"d\", value_name=\"sale\")\n",
    "\n",
    "# 和日期数据做关联\n",
    "data = pd.merge(data, calendar, how = 'left', \n",
    "                left_on = ['d'], right_on = ['d'])\n",
    "\n",
    "data = data[[\"id\", 'store_id', 'item_id', \"sale\", \"d\", \"wm_yr_wk\"]]\n",
    "\n",
    "# 和价格数据关联\n",
    "data = data.merge(sell_prices, on = ['store_id', 'item_id', 'wm_yr_wk'], how = 'left')\n",
    "data.drop(columns = ['wm_yr_wk'], inplace=True)\n",
    "\n",
    "# 计算售卖量\n",
    "data['sale_usd'] = data['sale'] * data['sell_price']\n",
    "\n",
    "# 得到聚合矩阵\n",
    "# 30490 -> 42840\n",
    "# 需要聚合的维度明细计算出来\n",
    "dummies_list = [sales.state_id, sales.store_id, \n",
    "                sales.cat_id, sales.dept_id, \n",
    "                sales.state_id + sales.cat_id, sales.state_id + sales.dept_id,\n",
    "                sales.store_id + sales.cat_id, sales.store_id + sales.dept_id, \n",
    "                sales.item_id, sales.state_id + sales.item_id, sales.id]\n",
    "\n",
    "\n",
    "# 全部聚合为一个， 最高 level\n",
    "dummies_df_list =[pd.DataFrame(np.ones(sales.shape[0]).astype(np.int8), \n",
    "                               index=sales.index, columns=['all']).T]\n",
    "\n",
    "# 挨个计算其他 level 等级聚合\n",
    "for i, cats in enumerate(dummies_list):\n",
    "    dummies_df_list +=[pd.get_dummies(cats, drop_first=False, dtype=np.int8).T]\n",
    "    \n",
    "# 得到聚合矩阵\n",
    "roll_mat_df = pd.concat(dummies_df_list, keys=list(range(12)), \n",
    "                        names=['level','id'])#.astype(np.int8, copy=False)\n",
    "\n",
    "# 保存聚合矩阵\n",
    "roll_index = roll_mat_df.index\n",
    "roll_mat_csr = csr_matrix(roll_mat_df.values)\n",
    "roll_mat_df.to_pickle('roll_mat_df.pkl')\n",
    "\n",
    "# 释放内存\n",
    "del dummies_df_list, roll_mat_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照定义，计算每条时间序列 RMSSE 的权重:\n",
    "def get_s(drop_days=0):\n",
    "    \n",
    "    \"\"\"\n",
    "    drop_days: int, equals 0 by default, so S is calculated on all data.\n",
    "               If equals 28, last 28 days won't be used in calculating S.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 要计算的时间序列长度\n",
    "    d_name = ['d_' + str(i+1) for i in range(1913-drop_days)]\n",
    "    # 得到聚合结果\n",
    "    sales_train_val = roll_mat_csr * sales[d_name].values\n",
    "\n",
    "    # 按照定义，前面连续为 0 的不参与计算\n",
    "    start_no = np.argmax(sales_train_val>0, axis=1)\n",
    "    \n",
    "    # 这些连续为 0 的设置为 nan\n",
    "    flag = np.dot(np.diag(1/(start_no+1)) , np.tile(np.arange(1,1914-drop_days),(roll_mat_csr.shape[0],1)))<1\n",
    "    sales_train_val = np.where(flag, np.nan, sales_train_val)\n",
    "\n",
    "    # 根据公式计算每条时间序列 rmsse的权重\n",
    "    weight1 = np.nansum(np.diff(sales_train_val,axis=1)**2,axis=1)/(1913-start_no-1)\n",
    "    \n",
    "    return weight1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = get_s(drop_days=0)\n",
    "\n",
    "# 根据定义计算 WRMSSE 的权重，这里指 w \n",
    "def get_w(sale_usd):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # 得到最细维度的每条时间序列的权重\n",
    "    total_sales_usd = sale_usd.groupby(\n",
    "        ['id'], sort=False)['sale_usd'].apply(np.sum).values\n",
    "    \n",
    "    # 通过聚合矩阵得到不同聚合下的权重\n",
    "    weight2 = roll_mat_csr * total_sales_usd\n",
    "    \n",
    "    return 12*weight2/np.sum(weight2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = get_w(data[['id','sale_usd']])\n",
    "\n",
    "SW = W/np.sqrt(S)\n",
    "\n",
    "sw_df = pd.DataFrame(np.stack((S, W, SW), axis=-1),index = roll_index,columns=['s','w','sw'])\n",
    "sw_df.to_pickle('sw_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评分函数\n",
    "# 得到聚合的结果\n",
    "def rollup(v):\n",
    "    '''\n",
    "    '''\n",
    "    return (v.T*roll_mat_csr.T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算 WRMSSE 评估指标\n",
    "def wrmsse(preds, y_true, score_only=False, s = S, w = W, sw=SW):\n",
    "    '''\n",
    "    preds - Predictions: pd.DataFrame of size (30490 rows, N day columns)\n",
    "    y_true - True values: pd.DataFrame of size (30490 rows, N day columns)\n",
    "    sequence_length - np.array of size (42840,)\n",
    "    sales_weight - sales weights based on last 28 days: np.array (42840,)\n",
    "    '''\n",
    "    \n",
    "    if score_only:\n",
    "        return np.sum(\n",
    "                np.sqrt(\n",
    "                    np.mean(\n",
    "                        np.square(rollup(preds.values-y_true.values))\n",
    "                            ,axis=1)) * sw *12)\n",
    "    else: \n",
    "        score_matrix = (np.square(rollup(preds.values-y_true.values)) * np.square(w)[:, None]) *12 / s[:, None]\n",
    "        score = np.sum(np.sqrt(np.mean(score_matrix,axis=1)))\n",
    "        return score, score_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载前面预先计算好的各个权重\n",
    "file_pass = './'\n",
    "sw_df = pd.read_pickle(file_pass+'sw_df.pkl')\n",
    "S = sw_df.s.values\n",
    "W = sw_df.w.values\n",
    "SW = sw_df.sw.values\n",
    "\n",
    "roll_mat_df = pd.read_pickle(file_pass+'roll_mat_df.pkl')\n",
    "roll_index = roll_mat_df.index\n",
    "roll_mat_csr = csr_matrix(roll_mat_df.values)\n",
    "\n",
    "print(sw_df.loc[(11,slice(None))].sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(sw_df.loc[(11,slice(None))].sw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
